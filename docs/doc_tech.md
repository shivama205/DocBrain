# DocBrain Technology Architecture

This document outlines the architecture of the DocBrain system, detailing its components, the RAG (Retrieval-Augmented Generation) pipeline implementation, current limitations, and ideas for future work.

## 1. System Overview

DocBrain is an AI-powered document management and chat system that leverages a combination of modern technologies to ingest, process, store, and retrieve information. The system uses a MySQL database via SQLAlchemy as an ORM for storing metadata and seed data, and Pinecone for managing and searching vector embeddings. Additionally, we incorporate Gemini for generating responses and query processing.

Key components include:

- **Database and Models**: SQLAlchemy models define entities such as users, documents, knowledge bases, conversations, and messages. The database is implemented in MySQL and initialized with seed data.
- **Vector Store**: Pinecone is used to store the vector embeddings of document chunks generated by Gemini.
- **RAG Pipeline**: The Retrieval-Augmented Generation pipeline processes user queries by first retrieving relevant document chunks, optionally rewriting queries if no results are found, and finally generating responses using Gemini.
- **API Layer**: The system exposes services via FastAPI, handling requests for document uploads, query processing, and chat interactions.

## 2. Detailed Architecture Components

### 2.1 Database and ORM

- **SQLAlchemy Models**: Entities such as `User`, `KnowledgeBase`, `Document`, `Conversation`, and `Message` are implemented in SQLAlchemy. 
  - Models enforce data integrity through relationships and constraints. 
  - JSON fields are used where MySQL does not support ARRAY types directly. 
- **Database Initialization**: A script (`scripts/init_db_with_seed.py`) creates the MySQL database, tables, and initial seed data, ensuring that an admin user, test user, sample knowledge bases, documents, and conversations are pre-populated.

### 2.2 Vector Store and Gemini Integration

- **Pinecone Integration**: 
  - The `VectorRepository` class encapsulates operations to add, delete, and search vector embeddings within Pinecone.
  - Each document is divided into chunks and their corresponding embeddings are generated by Gemini. These embeddings are stored in Pinecone for vector similarity search.

- **Gemini Usage**:
  - **Embeddings**: Send text content to Gemini to generate 768-dimensional embeddings using the `text-embedding-004` model.
  - **Query Processing & Generation**: Gemini is employed to rewrite queries if no relevant chunks are retrieved, classify query types, generate query variations, and produce sub-questions to improve the retrieval phase. It is also used to generate the final response based on retrieved chunks.

### 2.3 RAG Pipeline

The Retrieval-Augmented Generation (RAG) pipeline is a core component of DocBrain. It operates in several stages:

1. **Initial Retrieval**:
   - A user query is passed to the `process_query` method in the RAG service.
   - The pipeline first attempts to retrieve relevant document chunks using the original query, by querying Pinecone for similar embeddings.
   - If relevant chunks are found, they sent to the next stage.

2. **Query Analysis and Rewriting**:
   - If no relevant chunks are retrieved initially, the system falls back to analyzing and rewriting the query.
   - This involves using Gemini to classify the query type (e.g., factoid, explanation, comparison) and generating variations and sub-questions to broaden the search context.
   - The rewritten query and its associated metadata (like query type and sub-questions) are then used for a second round of retrieval in Pinecone.

3. **Answer Generation**:
   - Once a set of relevant chunks is established, these chunks (with metadata like document title, section, and relevance score) are formatted to construct a comprehensive prompt.
   - Gemini is then tasked to generate an informative, clear, and direct answer using this prompt. The generated answer references specific sources (using a [Source X] notation) cementing the response on retrieved data.

4. **Output Delivery**:
   - Finally, the answer and the sources (chunks) are returned to the user, completing the RAG cycle.

### 2.4 API Layer and Services

- The system uses FastAPI to expose endpoints for various functionalities such as file upload, query processing, and chat interactions.
- The RAG service integrates with both the vector repository and the Gemini model to process queries and generate responses.

## 3. Current Limitations

While DocBrain has a robust architecture, there are some present limitations:

- **Dependency on External APIs**: Gemini and Pinecone are external services. Their performance, availability, and API changes can affect system stability.
- **Query Rewriting Accuracy**: The current query rewriting depends heavily on prompt engineering with Gemini. Sometimes, the rewritten query may not perform significantly better than the original due to nuanced differences in phrasing.
- **Scalability of Vector Retrieval**: As the number of documents grows, the performance of vector retrieval might be impacted. Although Pinecone is optimized for this, fine-tuning of parameters and cost management remains a concern.
- **Limited Data Migration**: Currently, data migration and synchronization between different storage systems (MySQL and Pinecone) are manual.
- **Basic Error Handling**: Error logging and handling, while present, could be improved to provide more granular context and recovery options.

## 4. Future Work

To further enhance the system, the following improvements are planned:

- **Enhanced Query Rewriting and AI Tuning**: Improve the prompts and integration with Gemini to produce more accurate query variations and better results.
- **Scalability Improvements**: Explore advanced indexing techniques or alternative vector databases as the dataset grows, to ensure efficient retrieval.
- **Advanced Analytics and Logging**: Implement more detailed monitoring and logging for both the RAG pipeline and the database, helping to quickly resolve issues.
- **Data Migration Tools**: Develop automated tools for migrating data between different vector stores or databases to facilitate scaling and redundancy.
- **User Personalization**: Integrate more personalization and context-aware search capabilities to tailor responses based on user history and preferences.
- **Robust Error Handling**: Enhance exception management and provide fallback mechanisms to ensure the system remains responsive during external API failures or high load.

## 5. Conclusion

DocBrain is a cutting-edge, hybrid AI system that leverages the power of vector databases and generative AI in the RAG pipeline to deliver rich, contextual responses. While the architecture is modular and scalable, focusing on continuous improvement and error handling will be key to evolving the platform into a production-grade solution. 